Step-by-step to submit Spark job using your cde binary and TLS certs
1. Verify cde binary

From your extracted path (/root/cde-11-06/cde-env-tool):

cd /root/cde-11-06/cde-env-tool
./cde version


If it prints a version (e.g., 1.22.2 or similar), you’re good.

If you get “permission denied,” fix it:

chmod 755 ./cde

2. Prepare CDE credentials

Create directory:

mkdir -p ~/.cde


Now create a file:

vi ~/.cde/credentials


Add your certs (replace paths with actual):

client-cert: /root/certs-11-03-2025/cde-k4sg67rt.pem
client-key:  /root/certs-11-03-2025/cde-k4sg67rt.key


Save and close.

3. Create your config file
vi ~/.cde/config.yaml


Example (replace <vc-endpoint> with your CDE Virtual Cluster Jobs API endpoint):

allow-all-spark-submit-flags: true
credentials-file: /root/.cde/credentials
tls-insecure: true     # optional, if your internal cert isn’t trusted
profiles:
  - name: default
    vcluster-endpoint: https://console-ds-17oct.apps.mlab-ctigtcdai03d.ecs.dyn.nsroot.net/dex/api/v1


You can find your vcluster endpoint in the CDE UI → Administration → Virtual Clusters → (your cluster) → Jobs API URL.

4. Test connection

Run:

./cde cluster describe


If it connects successfully, your certs and endpoint are good.

5. Upload a Spark example script

Let’s make a quick test script:

cat << 'EOF' > sparkpi.py
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("PiExample").getOrCreate()
n = 100000
count = spark.sparkContext.parallelize(range(1, n+1)).filter(lambda x: (x*x + (n-x)*(n-x)) < n*n).count()
print("Approx Pi =", 4.0 * count / n)
spark.stop()
EOF


Now create and upload a resource:

./cde resource create --name test-scripts --type files
./cde resource upload --name test-scripts --local-path sparkpi.py

6. Create a job
./cde job create \
  --name spark-pi-test \
  --type spark \
  --mount-1-resource test-scripts \
  --application-file sparkpi.py \
  --driver-cores 1 \
  --driver-memory 1g \
  --executor-cores 1 \
  --executor-memory 1g \
  --num-executors 2

7. Run the job
./cde job run --name spark-pi-test --wait


You’ll see job progress and final status (e.g., Job run completed successfully).

8. View logs
./cde run logs --name spark-pi-test


or by run ID:

./cde run describe --id <run-id>

⚙️ Optional REST method (no CLI)

If you ever want to hit the API directly:

curl -X POST https://<vcluster-endpoint>/jobs/spark-pi-test/run \
--cert /root/certs-11-03-2025/cde-k4sg67rt.pem \
--key /root/certs-11-03-2025/cde-k4sg67rt.key \
--header "Content-Type: application/json"


==========


✅ Step 3 — Create ~/.cde/config.yaml

Run:

mkdir -p ~/.cde
vi ~/.cde/config.yaml


Paste exactly:

allow-all-spark-submit-flags: true
credentials-file: /root/.cde/credentials
tls-insecure: true       # keep if internal CA is untrusted
profiles:
  - name: default
    vcluster-endpoint: https://4n9wpd5l.cde-k4sg67rt.apps.mlab-ctigtdcai03d.ecs.dyn.nsroot.net/dex/api/v1


Save (:wq).

✅ Step 4 — Test connectivity

Run:

cd /root/cde-11-06/cde-env-tool
./cde cluster describe


If connection works, you’ll see details of the cluster and Spark 3.2.3 runtime.
If SSL errors occur, confirm tls-insecure: true is in your config and rerun.

✅ Step 5 — Upload the Spark examples JAR as a resource

Create the resource and upload the jar:

./cde resource create --name spark-examples --type files
./cde resource upload --name spark-examples --local-path spark-examples_2.11-2.4.8.7.1.9.1045-5.jar


Verify:

./cde resource describe --name spark-examples

✅ Step 6 — Create Spark Pi job

Run:

./cde job create \
  --name spark-pi-test \
  --type spark \
  --mount-1-resource spark-examples \
  --jar spark-examples_2.11-2.4.8.7.1.9.1045-5.jar \
  --class org.apache.spark.examples.SparkPi \
  --arg 10 \
  --driver-cores 1 \
  --driver-memory 1g \
  --executor-cores 1 \
  --executor-memory 1g \
  --num-executors 2


If you re-run and it already exists, update instead:

./cde job update --name spark-pi-test \
  --jar spark-examples_2.11-2.4.8.7.1.9.1045-5.jar \
  --class org.apache.spark.examples.SparkPi \
  --arg 10

✅ Step 7 — Submit the job and wait for completion
./cde job run --name spark-pi-test --wait


You should see output similar to:

Job run submitted: <run-id>
Job run status: running
Job run completed successfully

✅ Step 8 — Check logs and job details
./cde run logs --name spark-pi-test
# or by run ID:
./cde run describe --id <run-id>


You should see a line such as:

Pi is roughly 3.14xxxx

✅ Step 9 — (Alternative) Submit directly via API curl call

If you want to confirm your TLS-auth works directly:

curl -X POST \
  "https://4n9wpd5l.cde-k4sg67rt.apps.mlab-ctigtdcai03d.ecs.dyn.nsroot.net/dex/api/v1/jobs/spark-pi-test/run" \
  --cert /root/certs-11-03-2025/cde-k4sg67rt.pem \
  --key /root/certs-11-03-2025/cde-k4sg67rt.key \
  -H "Content-Type: application/json"


This returns a JSON payload with runId.



=========


Step-by-step fix

1️⃣ Edit your config file

vi ~/.cde/config.yaml


Replace your current content with this:

allow-all-spark-submit-flags: true
credentials-file: /root/.cde/credentials
tls-insecure: true   # keep if internal CA is untrusted
profiles:
  - name: vc-k4sg67rt
    vcluster-endpoint: https://4n9wpd5l.cde-k4sg67rt.apps.mlab-ctigtdcai03d.ecs.dyn.nsroot.net/dex/api/v1


Save and exit (:wq).

2️⃣ Test again specifying your new profile

cd /root/cde-11-06/cde-env-tool
./cde cluster describe --profile vc-k4sg67rt


You should now see cluster metadata (name, Spark version, and status).
If that works, everything from Step 3 onward (resource upload, job create, job run) will proceed successfully using:

./cde job create --profile vc-k4sg67rt ...
./cde job run --profile vc-k4sg67rt ...
